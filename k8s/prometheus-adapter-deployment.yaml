# References from: https://github.com/kubernetes-sigs/prometheus-adapter/tree/9156bf3fbc94337dec0a608f66404741e8581a77/deploy

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: adapter-config
  namespace: monitoring
data:
  config.yaml: |
    rules:
    # - seriesQuery: '{__name__=~"^container_.*",container_name!="POD",namespace!="",pod_name!=""}'
    #   seriesFilters: []
    #   resources:
    #     overrides:
    #       namespace:
    #         resource: namespace
    #       pod_name:
    #         resource: pod
    #   name:
    #     matches: ^container_(.*)_seconds_total$
    #     as: ""
    #   metricsQuery: sum(rate(<<.Series>>{<<.LabelMatchers>>,container_name!="POD"}[1m])) by (<<.GroupBy>>)
    # - seriesQuery: '{__name__=~"^container_.*",container_name!="POD",namespace!="",pod_name!=""}'
    #   seriesFilters:
    #   - isNot: ^container_.*_seconds_total$
    #   resources:
    #     overrides:
    #       namespace:
    #         resource: namespace
    #       pod_name:
    #         resource: pod
    #   name:
    #     matches: ^container_(.*)_total$
    #     as: ""
    #   metricsQuery: sum(rate(<<.Series>>{<<.LabelMatchers>>,container_name!="POD"}[1m])) by (<<.GroupBy>>)
    # - seriesQuery: '{__name__=~"^container_.*",container_name!="POD",namespace!="",pod_name!=""}'
    #   seriesFilters:
    #   - isNot: ^container_.*_total$
    #   resources:
    #     overrides:
    #       namespace:
    #         resource: namespace
    #       pod_name:
    #         resource: pod
    #   name:
    #     matches: ^container_(.*)$
    #     as: ""
    #   metricsQuery: sum(<<.Series>>{<<.LabelMatchers>>,container_name!="POD"}) by (<<.GroupBy>>)
    # - seriesQuery: '{namespace!="",__name__!~"^container_.*"}'
    #   seriesFilters:
    #   - isNot: .*_total$
    #   resources:
    #     template: <<.Resource>>
    #   name:
    #     matches: ""
    #     as: ""
    #   metricsQuery: sum(<<.Series>>{<<.LabelMatchers>>}) by (<<.GroupBy>>)
    # - seriesQuery: '{namespace!="",__name__!~"^container_.*"}'
    #   seriesFilters:
    #   - isNot: .*_seconds_total
    #   resources:
    #     template: <<.Resource>>
    #   name:
    #     matches: ^(.*)_total$
    #     as: ""
    #   metricsQuery: sum(rate(<<.Series>>{<<.LabelMatchers>>}[1m])) by (<<.GroupBy>>)
    # - seriesQuery: '{namespace!="",__name__!~"^container_.*"}'
    #   seriesFilters: []
    #   resources:
    #     template: <<.Resource>>
    #   name:
    #     matches: ^(.*)_seconds_total$
    #     as: ""
    #   metricsQuery: sum(rate(<<.Series>>{<<.LabelMatchers>>}[1m])) by (<<.GroupBy>>)
    # this works kinda
    - seriesQuery: '{__name__="rabbitmq_queue_messages"}'
      seriesFilters: []
      resources:
        template: <<.Resource>>
      name:
        matches: "rabbitmq_queue_messages"
        as: "rabbitmq_queue_length"
      metricsQuery: sum(<<.Series>>{<<.LabelMatchers>>}) by (<<.GroupBy>>)
    # externalRules:
    # - seriesQuery: '{__name__="rabbitmq_queue_messages",queue!=""}'
    #   metricsQuery: sum(<<.Series>>{<<.LabelMatchers>>}) by (queue)
    #   resources:
    #     namespaced: false
    # resourceRules:
    #   cpu:
    #     containerQuery: sum(rate(container_cpu_usage_seconds_total{<<.LabelMatchers>>}[1m])) by (<<.GroupBy>>)
    #     nodeQuery: sum(rate(container_cpu_usage_seconds_total{<<.LabelMatchers>>, id='/'}[1m])) by (<<.GroupBy>>)
    #     resources:
    #       overrides:
    #         instance:
    #           resource: node
    #         namespace:
    #           resource: namespace
    #         pod_name:
    #           resource: pod
    #     containerLabel: container_name
    #   memory:
    #     containerQuery: sum(container_memory_working_set_bytes{<<.LabelMatchers>>}) by (<<.GroupBy>>)
    #     nodeQuery: sum(container_memory_working_set_bytes{<<.LabelMatchers>>,id='/'}) by (<<.GroupBy>>)
    #     resources:
    #       overrides:
    #         instance:
    #           resource: node
    #         namespace:
    #           resource: namespace
    #         pod_name:
    #           resource: pod
    #     containerLabel: container_name
    #   window: 1m
---
apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app: custom-metrics-apiserver
  name: custom-metrics-apiserver
  namespace: monitoring

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app: custom-metrics-apiserver
  name: resource-metrics-auth-reader
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: extension-apiserver-authentication-reader
subjects:
- kind: ServiceAccount
  name: custom-metrics-apiserver
  namespace: monitoring

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: hpa-controller-custom-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: custom-metrics-server-resources
subjects:
- kind: ServiceAccount
  name: horizontal-pod-autoscaler
  namespace: kube-system

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: custom-metrics:system:auth-delegator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:auth-delegator
subjects:
- kind: ServiceAccount
  name: custom-metrics-apiserver
  namespace: monitoring

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: custom-metrics-resource-reader
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: custom-metrics-resource-reader
subjects:
- kind: ServiceAccount
  name: custom-metrics-apiserver
  namespace: monitoring

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: custom-metrics-server-resources
rules:
- apiGroups:
  - custom.metrics.k8s.io
  resources: ["*"]
  verbs: ["*"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: custom-metrics-resource-reader
rules:
- apiGroups:
  - ""
  resources:
  - nodes
  - namespaces
  - pods
  - services
  verbs:
  - get
  - watch
  - list

---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: custom-metrics-apiserver
  name: custom-metrics-apiserver
  namespace: monitoring
spec:
  replicas: 2
  selector:
    matchLabels:
      app: custom-metrics-apiserver
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  template:
    metadata:
      labels:
        app: custom-metrics-apiserver
    spec:
      automountServiceAccountToken: true
      containers:
      - args:
        - --cert-dir=/var/run/serving-cert
        - --config=/etc/adapter/config.yaml
        - --metrics-relist-interval=1m
        - --prometheus-url=http://prometheus-service.monitoring.svc:9090/
        - --secure-port=6443
        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_RSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA
        image: registry.k8s.io/prometheus-adapter/prometheus-adapter:v0.11.2
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /livez
            port: https
            scheme: HTTPS
          initialDelaySeconds: 30
          periodSeconds: 5
        name: custom-metrics-apiserver
        ports:
        - containerPort: 6443
          name: https
        readinessProbe:
          failureThreshold: 5
          httpGet:
            path: /readyz
            port: https
            scheme: HTTPS
          initialDelaySeconds: 30
          periodSeconds: 5
        resources:
          limits:
            cpu: "500m"
            memory: "512Mi"
          requests:
            cpu: "100m"
            memory: "256Mi"
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
        terminationMessagePolicy: FallbackToLogsOnError
        volumeMounts:
        - mountPath: /tmp
          name: tmpfs
          readOnly: false
        - mountPath: /var/run/serving-cert
          name: volume-serving-cert
          readOnly: false
        - mountPath: /etc/adapter
          name: config
          readOnly: false
      nodeSelector:
        kubernetes.io/os: linux
      securityContext: {}
      serviceAccountName: custom-metrics-apiserver
      volumes:
      - emptyDir: {}
        name: tmpfs
      - emptyDir: {}
        name: volume-serving-cert
      - configMap:
          name: adapter-config
        name: config

---

apiVersion: v1
kind: Service
metadata:
  name: custom-metrics-apiserver
  namespace: monitoring
spec:
  ports:
  - port: 443
    targetPort: 6443
  selector:
    app: custom-metrics-apiserver
---
apiVersion: apiregistration.k8s.io/v1
kind: APIService
metadata:
  name: v1beta1.custom.metrics.k8s.io
spec:
  service:
    name: custom-metrics-apiserver
    namespace: monitoring
  group: custom.metrics.k8s.io
  version: v1beta1
  insecureSkipTLSVerify: true
  groupPriorityMinimum: 100
  versionPriority: 100







# ---
# apiVersion: v1
# automountServiceAccountToken: false
# kind: ServiceAccount
# metadata:
#   labels:
#     app: prometheus-adapter
#   name: prometheus-adapter
#   namespace: monitoring

# ---
# apiVersion: v1
# kind: Service
# metadata:
#   labels:
#     app: prometheus-adapter
#   name: prometheus-adapter
#   namespace: monitoring
# spec:
#   ports:
#   - name: https
#     port: 443
#     targetPort: 6443
#   selector:
#     app: prometheus-adapter

# ---
# apiVersion: apiregistration.k8s.io/v1
# kind: APIService
# metadata:
#   labels:
#     app: prometheus-adapter
#   name: v1beta1.custom.metrics.k8s.io
# spec:
#   group: custom.metrics.k8s.io
#   groupPriorityMinimum: 100
#   insecureSkipTLSVerify: true
#   service:
#     name: prometheus-adapter
#     namespace: monitoring
#   version: v1beta1
#   versionPriority: 100

# ---
# apiVersion: rbac.authorization.k8s.io/v1
# kind: ClusterRole
# metadata:
#   labels:
#     app: prometheus-adapter
#     rbac.authorization.k8s.io/aggregate-to-admin: "true"
#     rbac.authorization.k8s.io/aggregate-to-edit: "true"
#     rbac.authorization.k8s.io/aggregate-to-view: "true"
#   name: system:aggregated-metrics-reader
#   namespace: monitoring
# rules:
# - apiGroups:
#   - custom.metrics.k8s.io
#   resources:
#   - pods
#   - nodes
#   verbs:
#   - get
#   - list
#   - watch

# ---
# apiVersion: rbac.authorization.k8s.io/v1
# kind: ClusterRoleBinding
# metadata:
#   labels:
#     app: prometheus-adapter
#   name: resource-metrics:system:auth-delegator
#   namespace: monitoring
# roleRef:
#   apiGroup: rbac.authorization.k8s.io
#   kind: ClusterRole
#   name: system:auth-delegator
# subjects:
# - kind: ServiceAccount
#   name: prometheus-adapter
#   namespace: monitoring

# ---
# apiVersion: rbac.authorization.k8s.io/v1
# kind: ClusterRoleBinding
# metadata:
#   name: hpa-controller-custom-metrics
#   labels:
#     app: prometheus-adapter
# roleRef:
#   apiGroup: rbac.authorization.k8s.io
#   kind: ClusterRole
#   name: custom-metrics-server-resources
# subjects:
# - kind: ServiceAccount
#   name: horizontal-pod-autoscaler
#   namespace: kube-system

# ---
# apiVersion: rbac.authorization.k8s.io/v1
# kind: ClusterRoleBinding
# metadata:
#   labels:
#     app: prometheus-adapter
#   name: prometheus-adapter
#   namespace: monitoring
# roleRef:
#   apiGroup: rbac.authorization.k8s.io
#   kind: ClusterRole
#   name: prometheus-adapter
# subjects:
# - kind: ServiceAccount
#   name: prometheus-adapter
#   namespace: monitoring

# ---
# apiVersion: rbac.authorization.k8s.io/v1
# kind: ClusterRole
# metadata:
#   labels:
#     app: prometheus-adapter
#   name: resource-metrics-server-resources
# rules:
# - apiGroups:
#   - custom.metrics.k8s.io
#   resources:
#   - '*'
#   verbs:
#   - '*'

# ---
# apiVersion: rbac.authorization.k8s.io/v1
# kind: ClusterRole
# metadata:
#   labels:
#     app: prometheus-adapter
#   name: prometheus-adapter
# rules:
# - apiGroups:
#   - ""
#   resources:
#   - nodes
#   - namespaces
#   - pods
#   - services
#   verbs:
#   - get
#   - list
#   - watch

# ---
# apiVersion: v1
# data:
#   config.yaml: |-
#     rules:
#     externalRules:
#     - seriesQuery: '{__name__="rabbitmq_queue_messages",queue!=""}'
#       metricsQuery: sum(<<.Series>>{<<.LabelMatchers>>}) by (name)
#       resources:
#         namespaced: false
#     - seriesQuery: 'rabbitmq_queue_messages{queue!=""}'
#       name:
#         matches: "rabbitmq_queue_messages"
#         as: "rabbitmq_queue_length"
#       resources:
#         namespaced: false
#       metricsQuery: (sum(rate(<<.Series>>{<<.LabelMatchers>>}[1m])) by (<<.GroupBy>>))
#     "resourceRules":
#       "cpu":
#         "containerLabel": "container"
#         "containerQuery": |
#           sum by (<<.GroupBy>>) (
#             irate (
#                 container_cpu_usage_seconds_total{<<.LabelMatchers>>,container!="",pod!=""}[4m]
#             )
#           )
#         "nodeQuery": |
#           sum by (<<.GroupBy>>) (
#             irate(
#                 node_cpu_usage_seconds_total{<<.LabelMatchers>>}[4m]
#             )
#           )
#         "resources":
#           "overrides":
#             "namespace":
#               "resource": "namespace"
#             "node":
#               "resource": "node"
#             "pod":
#               "resource": "pod"
#       "memory":
#         "containerLabel": "container"
#         "containerQuery": |
#           sum by (<<.GroupBy>>) (
#             container_memory_working_set_bytes{<<.LabelMatchers>>,container!="",pod!=""}
#           )
#         "nodeQuery": |
#           sum by (<<.GroupBy>>) (
#             node_memory_working_set_bytes{<<.LabelMatchers>>}
#           )
#         "resources":
#           "overrides":
#             "node":
#               "resource": "node"
#             "namespace":
#               "resource": "namespace"
#             "pod":
#               "resource": "pod"
#       "window": "5m"
# kind: ConfigMap
# metadata:
#   labels:
#     app: prometheus-adapter
#   name: adapter-config
#   namespace: monitoring

# ---
# apiVersion: networking.k8s.io/v1
# kind: NetworkPolicy
# metadata:
#   labels:
#     app: prometheus-adapter
#   name: prometheus-adapter
#   namespace: monitoring
# spec:
#   egress:
#   - {}
#   ingress:
#   - {}
#   podSelector:
#     matchLabels:
#       app: prometheus-adapter
#   policyTypes:
#   - Egress
#   - Ingress

# ---
# apiVersion: policy/v1
# kind: PodDisruptionBudget
# metadata:
#   labels:
#     app: prometheus-adapter
#   name: prometheus-adapter
#   namespace: monitoring
# spec:
#   minAvailable: 1
#   selector:
#     matchLabels:
#       app: prometheus-adapter

# ---
# apiVersion: rbac.authorization.k8s.io/v1
# kind: RoleBinding
# metadata:
#   labels:
#     app: prometheus-adapter
#   name: resource-metrics-auth-reader
#   namespace: kube-system
# roleRef:
#   apiGroup: rbac.authorization.k8s.io
#   kind: Role
#   name: extension-apiserver-authentication-reader
# subjects:
# - kind: ServiceAccount
#   name: prometheus-adapter
#   namespace: monitoring

# ---
# apiVersion: apps/v1
# kind: Deployment
# metadata:
#   labels:
#     app: prometheus-adapter
#   name: prometheus-adapter
#   namespace: monitoring
# spec:
#   replicas: 2
#   selector:
#     matchLabels:
#       app: prometheus-adapter
#   strategy:
#     rollingUpdate:
#       maxSurge: 1
#       maxUnavailable: 1
#   template:
#     metadata:
#       labels:
#         app: prometheus-adapter
#     spec:
#       automountServiceAccountToken: true
#       containers:
#       - args:
#         - --cert-dir=/var/run/serving-cert
#         - --config=/etc/adapter/config.yaml
#         - --metrics-relist-interval=1m
#         - --prometheus-url=http://prometheus-service.monitoring.svc:9090/
#         - --secure-port=6443
#         - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_RSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA
#         image: registry.k8s.io/prometheus-adapter/prometheus-adapter:v0.11.2
#         livenessProbe:
#           failureThreshold: 5
#           httpGet:
#             path: /livez
#             port: https
#             scheme: HTTPS
#           initialDelaySeconds: 30
#           periodSeconds: 5
#         name: prometheus-adapter
#         ports:
#         - containerPort: 6443
#           name: https
#         readinessProbe:
#           failureThreshold: 5
#           httpGet:
#             path: /readyz
#             port: https
#             scheme: HTTPS
#           initialDelaySeconds: 30
#           periodSeconds: 5
#         resources:
#           limits:
#             cpu: "500m"
#             memory: "512Mi"
#           requests:
#             cpu: "100m"
#             memory: "256Mi"
#         securityContext:
#           allowPrivilegeEscalation: false
#           capabilities:
#             drop:
#             - ALL
#           readOnlyRootFilesystem: true
#         terminationMessagePolicy: FallbackToLogsOnError
#         volumeMounts:
#         - mountPath: /tmp
#           name: tmpfs
#           readOnly: false
#         - mountPath: /var/run/serving-cert
#           name: volume-serving-cert
#           readOnly: false
#         - mountPath: /etc/adapter
#           name: config
#           readOnly: false
#       nodeSelector:
#         kubernetes.io/os: linux
#       securityContext: {}
#       serviceAccountName: prometheus-adapter
#       volumes:
#       - emptyDir: {}
#         name: tmpfs
#       - emptyDir: {}
#         name: volume-serving-cert
#       - configMap:
#           name: adapter-config
#         name: config